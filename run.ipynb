{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/999 | Loss 10.1801 | Win count 0\n",
      "Epoch 001/999 | Loss 6.1748 | Win count 0\n",
      "Epoch 002/999 | Loss 7.1970 | Win count 0\n",
      "Epoch 003/999 | Loss 6.6862 | Win count 0\n",
      "Epoch 004/999 | Loss 6.3099 | Win count 0\n",
      "Epoch 005/999 | Loss 5.1816 | Win count 0\n",
      "Epoch 006/999 | Loss 7.7510 | Win count 0\n",
      "Epoch 007/999 | Loss 3.5384 | Win count 0\n",
      "Epoch 008/999 | Loss 4.4765 | Win count 0\n",
      "Epoch 009/999 | Loss 3.6764 | Win count 0\n",
      "Epoch 010/999 | Loss 9.8292 | Win count 0\n",
      "Epoch 011/999 | Loss 4.9087 | Win count 0\n",
      "Epoch 012/999 | Loss 6.0121 | Win count 0\n",
      "Epoch 013/999 | Loss 11.0233 | Win count 0\n",
      "Epoch 014/999 | Loss 5.8175 | Win count 0\n",
      "Epoch 015/999 | Loss 10.8819 | Win count 0\n",
      "Epoch 016/999 | Loss 11.9098 | Win count 0\n",
      "Epoch 017/999 | Loss 5.8775 | Win count 0\n",
      "Epoch 018/999 | Loss 8.0692 | Win count 0\n",
      "Epoch 019/999 | Loss 7.1373 | Win count 0\n",
      "Epoch 020/999 | Loss 8.6544 | Win count 0\n",
      "Epoch 021/999 | Loss 8.1624 | Win count 0\n",
      "Epoch 022/999 | Loss 5.4350 | Win count 0\n",
      "Epoch 023/999 | Loss 5.7397 | Win count 0\n",
      "Epoch 024/999 | Loss 4.6979 | Win count 0\n",
      "Epoch 025/999 | Loss 5.2897 | Win count 0\n",
      "Epoch 026/999 | Loss 5.9013 | Win count 0\n",
      "Epoch 027/999 | Loss 12.9477 | Win count 0\n",
      "Epoch 028/999 | Loss 5.5368 | Win count 0\n",
      "Epoch 029/999 | Loss 3.9994 | Win count 0\n",
      "Epoch 030/999 | Loss 9.1801 | Win count 0\n",
      "Epoch 031/999 | Loss 7.0082 | Win count 0\n",
      "Epoch 032/999 | Loss 9.8647 | Win count 0\n",
      "Epoch 033/999 | Loss 7.6368 | Win count 0\n",
      "Epoch 034/999 | Loss 4.9665 | Win count 0\n",
      "Epoch 035/999 | Loss 5.3251 | Win count 0\n",
      "Epoch 036/999 | Loss 9.5910 | Win count 0\n",
      "Epoch 037/999 | Loss 6.5982 | Win count 0\n",
      "Epoch 038/999 | Loss 7.9931 | Win count 0\n",
      "Epoch 039/999 | Loss 10.6612 | Win count 0\n",
      "Epoch 040/999 | Loss 9.2724 | Win count 0\n",
      "Epoch 041/999 | Loss 5.7522 | Win count 0\n",
      "Epoch 042/999 | Loss 4.6708 | Win count 0\n",
      "Epoch 043/999 | Loss 7.5609 | Win count 0\n",
      "Epoch 044/999 | Loss 5.7639 | Win count 0\n",
      "Epoch 045/999 | Loss 11.3081 | Win count 0\n",
      "Epoch 046/999 | Loss 13.4534 | Win count 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-87933a0c4eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# adapt model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-87933a0c4eee>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, model, batch_size)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# There should be no target values for actions not taken.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Thou shalt not correct actions not taken #deep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mQ_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if game_over is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "\n",
    "\n",
    "class Catch(object):\n",
    "    def __init__(self, grid_size):\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        \"\"\"\n",
    "        Input: action and states\n",
    "        Ouput: new states and reward\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        # 0 = left\n",
    "        # 1 = right\n",
    "        # 2 = down\n",
    "        # 3 = up\n",
    "        \n",
    "        fy, fx, py, px, t, d = state\n",
    "        \n",
    "        old_d = abs(fx - px) + abs(fy - py)\n",
    "        \n",
    "        if action == 0:\n",
    "            if px > 0:\n",
    "                px -= 1\n",
    "        if action == 1:\n",
    "            if px < self.grid_size-1:\n",
    "                px += 1\n",
    "        if action == 2:\n",
    "            if py > 0:\n",
    "                py-= 1\n",
    "        if action == 3:\n",
    "            if py < self.grid_size-1:\n",
    "                py += 1\n",
    "                \n",
    "        new_d = abs(fx - px) + abs(fy - py)\n",
    "        \n",
    "        out = np.array([fy, fx, py, px, t-1, old_d-new_d])\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        im_size = (self.grid_size,)*2\n",
    "        state = self.state\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[state[0], state[1]] = 1  # draw fruit\n",
    "        canvas[state[2], state[3]] = 0.5  # draw basket\n",
    "        return canvas\n",
    "\n",
    "    def _get_reward(self):\n",
    "        fruit_y, fruit_x, player_y, player_x, t, d = self.state\n",
    "        \n",
    "        if fruit_x == player_x and fruit_y == player_y:\n",
    "            return 1\n",
    "        \n",
    "        if d == 1:\n",
    "            return 0.1\n",
    "        \n",
    "        if d == 0:\n",
    "            return -0.1\n",
    "        \n",
    "        if d == -1:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    def _is_over(self):\n",
    "        fruit_y, fruit_x, player_y, player_x, t, d = self.state\n",
    "        \n",
    "        if t == 0:\n",
    "            return True\n",
    "        \n",
    "        if fruit_x == player_x and fruit_y == player_y:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, action):\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        return self.observe(), reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        fruit_x = np.random.randint(0, self.grid_size-1)\n",
    "        fruit_y = np.random.randint(0, self.grid_size-1)\n",
    "        player_x = np.random.randint(0, self.grid_size-1)\n",
    "        player_y = np.random.randint(0, self.grid_size-1)\n",
    "        time = abs(fruit_x - player_x) + abs(fruit_y - player_y)\n",
    "        time *= 2\n",
    "        \n",
    "        while abs(fruit_x - player_x) + abs(fruit_y - player_y) < self.grid_size/2:\n",
    "            fruit_x = np.random.randint(0, self.grid_size-1)\n",
    "            fruit_y = np.random.randint(0, self.grid_size-1)\n",
    "            player_x = np.random.randint(0, self.grid_size-1)\n",
    "            player_y = np.random.randint(0, self.grid_size-1)\n",
    "            time = abs(fruit_x - player_x) + abs(fruit_y - player_y)\n",
    "            time *= 2\n",
    "            \n",
    "        self.state = np.asarray([fruit_y, fruit_x, player_y, player_x, time, 0])\n",
    "\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i+1] = state_t\n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parameters\n",
    "    epsilon = .1  # exploration\n",
    "    num_actions = 4  # [move_left, stay, move_right]\n",
    "    epoch = 10000\n",
    "    max_memory = 500\n",
    "    hidden_size = 200\n",
    "    batch_size = 50\n",
    "    grid_size = 100\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(sgd(lr=.02), \"mse\")\n",
    "\n",
    "    # If you want to continue training from a previous model, just uncomment the line bellow\n",
    "    # model.load_weights(\"model_100.h5\")\n",
    "\n",
    "    # Define environment/game\n",
    "    env = Catch(grid_size)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "\n",
    "    # Train\n",
    "    win_cnt = 0\n",
    "    for e in range(epoch):\n",
    "        loss = 0.\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t = env.observe()\n",
    "\n",
    "        while not game_over:\n",
    "            input_tm1 = input_t\n",
    "            # get next action\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, num_actions, size=1)\n",
    "            else:\n",
    "                q = model.predict(input_tm1)\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            # apply action, get rewards and new state\n",
    "            input_t, reward, game_over = env.act(action)\n",
    "            if reward == 1:\n",
    "                win_cnt += 1\n",
    "\n",
    "            # store experience\n",
    "            exp_replay.remember([input_tm1, action, reward, input_t], game_over)\n",
    "\n",
    "            # adapt model\n",
    "            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "        print(\"Epoch {:03d}/999 | Loss {:.4f} | Win count {}\".format(e, loss, win_cnt))\n",
    "\n",
    "        # Save trained model weights and architecture, this will be used by the visualization code\n",
    "        model.save_weights(\"model_100.h5\", overwrite=True)\n",
    "        with open(\"model_100.json\", \"w\") as outfile:\n",
    "            json.dump(model.to_json(), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "82\n",
    "118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected dense_141_input to have shape (100,) but got array with shape (10000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-37fd65cd5a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# get next action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected dense_141_input to have shape (100,) but got array with shape (10000,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ce40ca2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Make sure this grid size matches the value used fro training\n",
    "    grid_size = 100\n",
    "\n",
    "    with open(\"model_100.json\", \"r\") as jfile:\n",
    "        model = model_from_json(json.load(jfile))\n",
    "    model.load_weights(\"model_100.h5\")\n",
    "    model.compile(\"sgd\", \"mse\")\n",
    "\n",
    "    # Define environment, game\n",
    "    env = Catch(grid_size)\n",
    "    c = 0\n",
    "    for e in range(10):\n",
    "        loss = 0.\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t = env.observe()\n",
    "\n",
    "        plt.imshow(input_t.reshape((grid_size,)*2),\n",
    "                   interpolation='none', cmap='gray')\n",
    "        plt.savefig(\"%03d.png\" % c)\n",
    "        c += 1\n",
    "        while not game_over:\n",
    "            input_tm1 = input_t\n",
    "\n",
    "            # get next action\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "            # apply action, get rewards and new state\n",
    "            input_t, reward, game_over = env.act(action)\n",
    "\n",
    "            plt.imshow(input_t.reshape((grid_size,)*2),\n",
    "                       interpolation='none', cmap='gray')\n",
    "            plt.savefig(\"%03d.png\" % c)\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ffmpeg -i %03d.png output.gif -vf fps=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
